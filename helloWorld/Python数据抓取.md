# python数据抓取
    本文档是做抓取数据的工作以来的一些经验和总结，适合那些刚入门数据抓取的同学
   

## 数据抓取策略技术架构演化 ( 复杂度：易 -> 难 )

    1、一般的、普通的网站，直接开个HttpClient抓取

    2、需要保持会话的，集成缓存中间件来抓取

    3、会被封ip网站，开始使用代理ip抓取

    4、使用代理ip，则可以用多线程抓取(不要太过，需要延时，防止抓取的目标网站崩溃)

    5、有多次跳转网页的才能抓取的，需要做好抓取的工具类

    6、如果网页是后加载的，则可以考虑用phantomjs等这类浏览器引擎来抓取网页

    7、目标网站做了js混淆、或者用phantomjs也抓取不到时，则可以考虑用  python + selenium + 浏览器 的方式抓取网页
    
    8、若目标网站对selenium方式做了webdriver的轨迹监测，可以使用pyppeteer来代替selenium + 浏览器方式


    

## 数据抓取策略具体分析

    一、数据抓取：

    1、先看列表页和详情页的内容是否规律

    2、先测试抓取列表页和详情页的数据，看是否有限制等

    3、如果有限制，则仔细分析浏览器请求时的异同，也可抓包进行分析

    4、模拟浏览器行为，再次进行数据抓取

    5、最少要弄四个互不影响、相互独立的功能：专门抓取列表页、专门解析列表页、专门抓取详情页、专门解析详情页

    6、如果有条件，则装个动态IP代理的软件，每隔一段时间换一次ip

    7、如有访问频次的限制，则每隔一段时间需要停止抓取（可自定义、可随机）

    8、如果ip被封，而且动态代理也不起作用，则可能需要另外找机器去抓取

    9、查看要抓取的网页，使用get 和 post的方式有何差异。有的网站对get方式无限制

    10、如果再抓取数据的几周后，突然抓不到了，但浏览器还能浏览要爬的网页，就得再次抓包分析

    11、如果要抓取的网站崩溃、或者宕机，要搞个心跳检测服务，来展示出改网站的崩溃规律，这样也能看出，改网站对单个ip限流的时间频率

    12、有的网站，一条条数据逐条抓取时可能没啥问题，但用代理ip多线程抓取时，可能会被认为是流量攻击或者DDOS攻击


    二、字段设计、数据处理、策略等：

    1、另外在抓取数据时，最好能明确要抓哪些条件下的数据，防止做无用功

    2、抓取到的数据可能需要经过处理、转化之后才能用。所以在最开始设计表的时候，就要考虑好每天增量数据同步的机制。把数据流的所有环节都定好大致的规则。防止后续为了同步而再加字段

    3、涉及到数据同步时，同步机制尽可能的简单


## 常见的反爬虫的策略：
    0、User-Agent + Referer检测、账号及Cookie验证
    1、访问一个页面，要进行多次跳转才得到源码，例如：中国土地市场网
    2、利用css3的自定义字体方式，使得抓取到的中文是乱码 ，例如：中国土地市场网
    3、达到一定的访问次数限制后，跳出验证码页面，例如：中国土地市场网
    4、达到一定的访问次数限制后，拉黑ip地址，例如：中国土地市场网
    5、关键的数据，使用很小像素的图片来代替(百度的OCR接口无法识别)，例如：国信中心房地产信息网
    6、达到一定的访问次数限制后,弹出滑动验证码，例如：安居客
    7、各种异步加载反复嵌套，例如：网易云音乐也怕爬什么都是异步加载嵌套在iframe里的，包括他的整个主页，而且src=”about:blank”
    8、前端的处理：网页看到的字符可能是SVG矢量图、background拼凑的、字符穿插的、伪元素隐藏式的、伪元素隐藏式的....
    9、用display:none来随机化网页源码，有网站还会随机类和id的名字再加点随机的trtd，更加不好捕捉.比如：全网代理ip
    10、部分微信公众号会穿插各种蜜汁字符，再用样式调整隐藏他们，比如他：叨逼姐说
    continue ......

## 数据处理
    去重处理、数据清洗、数据补充、数据更新

    一、去重处理
        url层面的去重，注意http和https
        数据层面的去重，如果目标网站有id的话，最好。没有的话，则需要比较主要的字段


    二、基础数据清洗分为两部分：格式化保存 和 数据纠正
    
    0、通用的字段必须规范化命名，如：主键id、省市区等三级、经纬度、名称、地址、创建时间、更新时间等字段，方便后续做批量化的刷数据
    
    1、抓取后的数据，部分字段必须要清洗才能使用：数字、小数、单位统一、日期格式

    2、清洗字段前，原始字段保留，用于校对清洗过程中的误差

    3、必须清洗的字段：省市区等三级（必须规范）、含有数字的、时间日期的、单位不统一的、

    4、如果抓取的数据中，如省市区数据明显有问题，则需做数据纠正处理

    5、数据解析完后，查看下每个字段的所有值，看看是否有异常

    三、数据补充
    1、抓回来的基础数据，可能还不太满足实际使用场景，可能需要根据已经抓取数据的名称，再去抓取补充数据
        例如：抓取到的学校信息里，需要补充：学校等级、在校生人数等

    2、抓回来基础数据可能不是来自政府、官方网站，导致数据不全、不完整的情况(特别是从全国性的网站抓到的)，看需要，按照逐个省份的相关网站去抓取
        例如：从学校大全里抓到的学校数据中，深圳市的数据有1600+，但是从广东省教育厅的数据中，深圳有2600+的学校，需要替换学校大全里原有的数据

    四、数据更新
        目标网站的数据中，某个字段或某些字段发生了变化，需要更新我们的已经抓取到的数据，要定时的去监控目标网站的数据变化

## 数据监控
    目标网站的数据量
    采集的数据量
    清洗后的数据量
    同步后的数据量
    转存后的数据量
    上线后的数据量










    